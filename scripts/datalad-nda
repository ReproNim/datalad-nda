#!/usr/bin/env python3
"""A helper to condition manifest from NDA for download

Somehow, unlike with awk/sed monster, I do not see any where file is pointed to
by different urls!

TODOs:
- ?NDA check if submission ID is chronological (we sort by it and download only latest)
- ?NDA what happens when file is REMOVED from one submission to another?!
  i.e. how could we tell that it was an incremental submission or
  "full re-submission"?
- Add a mode to do it only for a specific submission id?
  Then we could establish chronology of uploads!  Then we can fail if
  within the same submission we get multiple URLs for the same file!
  May be that is best to do within datalad crawler!? otherwise we need multiple runs
  while they go through different submission IDs.
- add option to split into subdatasets e.g. for each subject (based on a regex)
  or should we do it while piping into `datalad addurls`?
"""

__version__ = '0.1'

import click
import csv
import re
import sys
import stat

import os.path as op

from collections import defaultdict

import logging
# This would interfer with datalad logging, so we will just use DataLad's ways
#lgr = logging.getLogger(op.basename(__file__))
#logging.basicConfig(level=logging.INFO)
from datalad.log import LoggerHelper
lgr = LoggerHelper("datalad-nda").get_initialized_logger()

# Some constants which might becom options
url_regex = r's3://NDAR[^/]*/submission_[0-9]*/(?P<path>.*)'
url_field = 'associated_file'
# regular expressions, matching which on path would result in adding
# '/' after them to signal the dataset boundary
dataset_boundaries = [
    'derivatives/[^/]+',
    'sourcedata',
]


# To provide feedback on lengthy process of going through the large file
from tqdm import tqdm
import mmap
def get_num_lines(file_path):
    fp = open(file_path, "r+")
    buf = mmap.mmap(fp.fileno(), 0)
    lines = 0
    while buf.readline():
        lines += 1
    return lines


def cond1(row):
    """Basic conditioning of entries in row"""
    # we do not care about ""
    return [_.lstrip('"').rstrip('"') for _ in row]


def read_input(input):
    lgr.info("Reading entire file")
    lines = list(input)
    # lines = full_input.split('\n')
    lgr.info("Read %d lines", len(lines))
    tbl_reader = csv.reader(lines, delimiter="\t")
    header = cond1(next(tbl_reader))
    assert url_field in header
    # We will retain full record just in case,
    # and just add explicit
    recs = []
    submission_ids = set()
    dataset_ids = set()
    for i, row in tqdm(enumerate(tbl_reader), total=len(lines), leave=False):
        if i == 0 and row == header:
            lgr.debug("Skipping 2nd line - identical to first")
            continue
        row = cond1(row)
        rec = dict(zip(header, row))
        rec['submission_id'] = int(rec['submission_id'])
        submission_ids.add(rec['submission_id'])
        dataset_ids.add(rec['dataset_id'])
        recs.append(rec)
    lgr.info(f"Loaded {len(recs)} records from {len(submission_ids)} submissions for {len(dataset_ids)} datasets.")
    # assert len(dataset_ids) == 1   # AFAIK
    return header, recs


input_option = click.option('-i', '--input',
              default='-',
              help='Input "manifest" file from NDA.  Reads from sys.stdin if not provided',
              type=click.File('r'))
output_option = click.option('-o', '--output',
              default='-',
              help='Output .csv file to feed DataLad with.  Dumped to stdout if not provided',
              type=click.File('w'))


@click.command()
@input_option
def submissions(input):
    """Print submission ids"""
    _, recs = read_input(input)
    submissions = get_submissions(recs)
    lgr.info("%d unique submission ids found.  Printing in sorted order", len(submissions))
    print('\n'.join(map(str, submissions)))


def get_submissions(recs):
    submissions = sorted(set(
        r['submission_id']
        for r in tqdm(recs, total=len(recs), leave=False)))
    return submissions


@click.command()
@input_option
@output_option
@click.option('--submission',
              help='Consider only this specified submission ID',
              default=None)
def condition4datalad(input, output, submission):
    """'Condition' provided table so it could be fed into  datalad addurls
    """
    header, recs = read_input(input)

    out = get_conditioned(recs, submission)
    save_table(out, header, output)


def save_table(recs, header, output):
    writer = csv.DictWriter(output,  # dialect=csv.excel_tab,
                            fieldnames=header + ['path'])
    writer.writeheader()
    for rec in recs.values():
        writer.writerow(rec)
    lgr.info("Saved output to %s", output)


@click.command()
@input_option
@click.option("--fast", help="Use --fast git-annex mode", is_flag=True)
@click.option("-d", "--dataset", help="Path to the dataset", default=None)
@click.option("--develn", help="Number of files per submission to consider.  Only for development/test purposes",
              default=None, type=int)
def add2datalad(input, fast, dataset, develn):
    """'Condition' and add to datalad (dataset in PWD) all submissions
    """
    import datalad.api as dl
    from datalad.distribution.dataset import require_dataset
    from datalad.customremotes.base import init_datalad_remote

    ds = require_dataset(dataset, check_installed=False)

    if not ds.is_installed():
        lgr.info("Creating a new dataset at %s", ds.path)
        ds.create()

    if ds.repo.dirty:
        raise RuntimeError(f"{dataset} is dirty.  Please save all your changes first")

    # Prepare a procedure to be used for the datasets
    nda_proc_path = ds.pathobj / ".datalad" / "procedures" / "cfg_nda"
    if not nda_proc_path.exists():
        lgr.info("Creating a helper procedure")
        nda_proc_path.parent.mkdir()
        # TODO: think about windows people and redo in Python?
        nda_proc_path.write_text("""\
#!/bin/bash

set -eu

cd "$1"

git config core.splitIndex true
git config annex.retry 3
git annex initremote datalad externaltype=datalad type=external encryption=none autoenable=true
""" +
        "git config annex.security.allow-unverified-downloads ACKTHPPT" if fast else "")
        nda_proc_path.chmod(nda_proc_path.stat().st_mode | stat.S_IEXEC)
        ds.run_procedure('cfg_nda')

#    if "datalad" not in ds.repo.get_remotes():
#        init_datalad_remote(ds.repo, "datalad", autoenable=True)

    header, recs = read_input(input)
    submissions = get_submissions(recs)

    # Ensure that "sourcedata" is a subdataset
    if "sourcedata" not in ds.subdatasets(return_type='list', result_xfm='relpaths'):
        ds.create("sourcedata", cfg_proc="nda")

    # DataLad logging during addurls interfers with progressbars
    # logging.getLogger('datalad').setLevel(logging.WARNING)
    # but then seems to not show progressbars :-/

    cfg_var = 'datalad-nda.submissions.processed'
    for submission in submissions:
        submission = str(submission)  # ensure consistent dtype
        if submission in ds.config.get(cfg_var, ()):
            lgr.info("Skipping already processed submission %s", submission)
            continue
        lgr.info("Processing for submission %s", submission)
        ds.config.add(cfg_var, submission, where='dataset')
        submission_recs = get_conditioned(recs, submission)
        # ATM we cannot just pass a list into addurls -- it wants a filename or would need to
        # pipe via stdin, which is IMHO a bit too unpythonic for this script, so we will
        # just store that file on drive, thus somewhat duplicating information on what
        # submissions were processed as we store them in config as well.
        submission_file = ds.pathobj / "sourcedata" / ("submission-%s.csv" % submission)
        if not submission_file.parent.exists():
            submission_file.parent.mkdir()

        # if submission_file.lexists():
        #     submission_file.unlink()
        if develn:
            submission_recs = {k: v for i, (k, v) in enumerate(submission_recs.items()) if i < develn}

        with open(submission_file, 'w') as f:
            save_table(submission_recs, header, f)
        out = ds.addurls(
            submission_file, '{associated_file}', '{path}',
            exclude_autometa='*',
            fast=fast,
            save=False,
            ifexists="overwrite",
            cfg_proc='nda',
            on_failure="stop",
            # result_renderer='default'
        )
        # print(out)
        ds.save(message=f"Added submission {submission} with {len(submission_recs)} entries",
                recursive=True)


def get_conditioned(recs, submission):
    if submission:
        submission = int(submission)
        lgr.info("Getting records only for submission_id=%s", submission)
        recs = [r for r in tqdm(recs, total=len(recs), leave=False) if r['submission_id'] == submission]
        lgr.info("Got %d records", len(recs))
    else:
        lgr.info(f"Sorting (in reverse by submission_id) assuming that later submission ID is the most up to date one")
        recs = sorted(recs, key=lambda r: r["submission_id"], reverse=True)
    # in case of duplicates (the same url -> path) we keep only first hit record
    out = {}  # path: record.
    url_to_paths = defaultdict(set)  # just to check if some url (now) doesn't provide multiple files
    duplicates = defaultdict(list)  # just to keep internally list of all duplicates
    subdatasets = set()
    prev_submission_id = None
    for rec in tqdm(recs, leave=False):
        submission_id = rec['submission_id']
        if prev_submission_id is not None and prev_submission_id >= submission_id:
            raise ValueError(f"Not sorted by submission ID: {prev_submission_id} is followed by {submission_id}")

        url = rec[url_field]
        url_match = re.match(url_regex, url)
        if not url_match:
            raise ValueError(f"Cannot parse {url} using {url_regex!r}")
        path = url_match.groupdict()['path']
        if path in out:
            duplicates[path].append(rec)  # [url_field])
            known_url = out[path][url_field]
            if known_url != url:
                lgr.debug(f'{path}: {url} != {known_url}')
                continue
            if path not in url_to_paths.get(url, set()):
                # we saw that url already! but it had a different path
                raise ValueError(f'{url} already provided {url_to_paths[url]}')

        for dataset_boundary in dataset_boundaries:
            m = re.search(dataset_boundary, path)
            if m:
                b, e = m.span()
                assert path[e] == '/'
                path = '%s/%s' % (path[:e], path[e:])
                subdatasets.add(path[:e])
        url_to_paths[url].add(path)
        # for convenience store  that path straight in the rec
        rec['path'] = path
        out[path] = rec
    lgr.info(f"Processed entire file: got {len(recs)} files, {len(subdatasets)} subdatasets "
             f"with {len(duplicates)} files having multiple URLs to "
             f"possibly reach them")  # : {', '.join(duplicates)}")
    if len(duplicates):
        #multiurl_listing = '\n '.join(f' - {k}: {v}' for k, v in duplicates.items())
        multiurl_listing = '\n '.join(f' - {k}: {len(v)} records' for k, v in duplicates.items())
        lgr.warning(multiurl_listing)
    return out


class LogLevel(click.ParamType):
    name = "log-level"
    levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]

    def convert(self, value, param, ctx):
        if value is None:
            return value
        try:
            return int(value)
        except ValueError:
            vupper = value.upper()
            if vupper in self.levels:
                return getattr(logging, vupper)
            else:
                self.fail(f"{value!r}: invalid log level", param, ctx)

    def get_metavar(self, param):
        return "[" + "|".join(self.levels) + "]"


def is_interactive():
    """Return True if all in/outs are tty"""
    # TODO: check on windows if hasattr check would work correctly and add value:
    #
    return sys.stdin.isatty() and sys.stdout.isatty() and sys.stderr.isatty()


def setup_exceptionhook(ipython=False):
    """Overloads default sys.excepthook with our exceptionhook handler.

       If interactive, our exceptionhook handler will invoke
       pdb.post_mortem; if not interactive, then invokes default handler.
    """

    def _pdb_excepthook(type, value, tb):
        import traceback

        traceback.print_exception(type, value, tb)
        print()
        if is_interactive():
            import pdb

            pdb.post_mortem(tb)

    if ipython:
        from IPython.core import ultratb

        sys.excepthook = ultratb.FormattedTB(
            mode="Verbose",
            # color_scheme='Linux',
            call_pdb=is_interactive(),
        )
    else:
        sys.excepthook = _pdb_excepthook


#
# Main group
#


def print_version(ctx, param, value):
    if not value or ctx.resilient_parsing:
        return
    click.echo(__version__)
    ctx.exit()


def upper(ctx, param, value):
    import pdb

    pdb.set_trace()
    return value.upper()


# group to provide commands
@click.group() # cls=DYMGroup)
@click.option(
    "--version", is_flag=True, callback=print_version, expose_value=False, is_eager=True
)
@click.option(
    "-l",
    "--log-level",
    help="Log level (case insensitive).  May be specified as an integer.",
    type=LogLevel(),
    default="INFO",
    show_default=True,
)
@click.option("--pdb", help="Fall into pdb if errors out", is_flag=True)
def main(log_level, pdb=False):
    """Helpers to support interfacing DataLad to NDA

    To see help for a specific command, run

        datalad-nda COMMAND --help

    """
    lgr.setLevel(log_level)
    if pdb:
        setup_exceptionhook()
    # try:
    #     import etelemetry
    #
    #     etelemetry.check_available_version("dandi/dandi-cli", __version__, lgr=lgr)
    # except Exception as exc:
    #     lgr.warning(
    #         "Failed to check for a more recent version available with etelemetry: %s",
    #         exc,
    #     )


main.add_command(add2datalad)
main.add_command(condition4datalad)
main.add_command(submissions)

if __name__ == '__main__':
    main()
