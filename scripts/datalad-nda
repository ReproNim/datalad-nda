#!/usr/bin/env python3
"""A helper to condition manifest from NDA for download

Somehow, unlike with awk/sed monster, I do not see any where file is pointed to
by different urls!

TODOs:
- ?NDA check if submission ID is chronological (we sort by it and download only latest)
- ?NDA what happens when file is REMOVED from one submission to another?!
  i.e. how could we tell that it was an incremental submission or
  "full re-submission"?
- Add a mode to do it only for a specific submission id?
  Then we could establish chronology of uploads!  Then we can fail if
  within the same submission we get multiple URLs for the same file!
  May be that is best to do within datalad crawler!? otherwise we need multiple runs
  while they go through different submission IDs.
- add option to split into subdatasets e.g. for each subject (based on a regex)
  or should we do it while piping into `datalad addurls`?
"""

__version__ = '0.1'

import click
import csv
import re
import sys

from collections import defaultdict

import logging
lgr = logging.getLogger(__file__)
logging.basicConfig(level=logging.INFO)

# Some constants which might becom options
url_regex = r's3://NDAR[^/]*/submission_[0-9]*/(?P<path>.*)'
url_field = 'associated_file'
# regular expressions, matching which on path would result in adding
# '/' after them to signal the dataset boundary
dataset_boundaries = [
    'derivatives/[^/]+'
]


# To provide feedback on lengthy process of going through the large file
from tqdm import tqdm
import mmap
def get_num_lines(file_path):
    fp = open(file_path, "r+")
    buf = mmap.mmap(fp.fileno(), 0)
    lines = 0
    while buf.readline():
        lines += 1
    return lines


def cond1(row):
    """Basic conditioning of entries in row"""
    # we do not care about ""
    return [_.lstrip('"').rstrip('"') for _ in row]


def read_input(input):
    lgr.info("Reading entire file")
    lines = list(input)
    # lines = full_input.split('\n')
    lgr.info("Read %d lines", len(lines))
    tbl_reader = csv.reader(lines, delimiter="\t")
    header = cond1(next(tbl_reader))
    assert url_field in header
    # We will retain full record just in case,
    # and just add explicit
    recs = []
    submission_ids = set()
    dataset_ids = set()
    for i, row in tqdm(enumerate(tbl_reader), total=len(lines), leave=False):
        if i == 0 and row == header:
            lgr.debug("Skipping 2nd line - identical to first")
            continue
        row = cond1(row)
        rec = dict(zip(header, row))
        rec['submission_id'] = int(rec['submission_id'])
        submission_ids.add(rec['submission_id'])
        dataset_ids.add(rec['dataset_id'])
        recs.append(rec)
    lgr.info(f"Loaded {len(recs)} records from {len(submission_ids)} submissions for {len(dataset_ids)} datasets.")
    # assert len(dataset_ids) == 1   # AFAIK
    return header, recs


input_option = click.option('-i', '--input',
              default='-',
              help='Input "manifest" file from NDA.  Reads from sys.stdin if not provided',
              type=click.File('r'))
output_option = click.option('-o', '--output',
              default='-',
              help='Output .csv file to feed DataLad with.  Dumped to stdout if not provided',
              type=click.File('w'))


@click.command()
@input_option
def submissions(input):
    """Print submission ids"""
    header, recs = read_input(input)
    submissions = sorted(set(
        r['submission_id']
        for r in tqdm(recs, total=len(recs), leave=False)))
    lgr.info("%d unique submission ids found.  Printing in sorted order", len(submissions))
    print('\n'.join(map(str, submissions)))


@click.command()
@input_option
@output_option
@click.option('--submission',
              help='Consider only this specified submission ID',
              default=None)
def condition4datalad(input, output, submission):
    """'Condition' provided table so it could be fed into  datalad addurls
    """
    header, recs = read_input(input)

    if submission:
        submission = int(submission)
        lgr.info("Getting records only for submission_id=%s", submission)
        recs = [r for r in tqdm(recs, total=len(recs), leave=False) if r['submission_id'] == submission]
        lgr.info("Got %d records", len(recs))
    else:
        lgr.info(f"Sorting (in reverse by submission_id) assuming that later submission ID is the most up to date one")
        recs = sorted(recs, key=lambda r: r["submission_id"], reverse=True)

    # in case of duplicates (the same url -> path) we keep only first hit record
    out = {}  # path: record.
    url_to_paths = defaultdict(set)  # just to check if some url (now) doesn't provide multiple files
    duplicates = defaultdict(list)  # just to keep internally list of all duplicates
    subdatasets = set()
    prev_submission_id = None
    for rec in tqdm(recs, leave=False):
        submission_id = rec['submission_id']
        if prev_submission_id is not None and prev_submission_id >= submission_id:
            raise ValueError(f"Not sorted by submission ID: {prev_submission_id} is followed by {submission_id}")

        url = rec[url_field]
        url_match = re.match(url_regex, url)
        if not url_match:
            raise ValueError(f"Cannot parse {url} using {url_regex!r}")
        path = url_match.groupdict()['path']
        if path in out:
            duplicates[path].append(rec) # [url_field])
            known_url = out[path][url_field]
            if known_url != url:
                lgr.debug(f'{path}: {url} != {known_url}')
                continue
            if path not in url_to_paths.get(url, set()):
                # we saw that url already! but it had a different path
                raise ValueError(f'{url} already provided {url_to_paths[url]}')

        for dataset_boundary in dataset_boundaries:
            m = re.search(dataset_boundary, path)
            if m:
                b, e = m.span()
                assert path[e] == '/'
                path = '%s/%s' % (path[:e], path[e:])
                subdatasets.add(path[:e])
        url_to_paths[url].add(path)
        # for convenience store  that path straight in the rec
        rec['path'] = path
        out[path] = rec
    #multiurl_listing = '\n '.join(f' - {k}: {v}' for k, v in duplicates.items())
    lgr.info(f"Processed entire file: got {len(recs)} files, {len(subdatasets)} subdatasets "
             f"with {len(duplicates)} files having multiple URLs to "
             f"possibly reach them")  # : {', '.join(duplicates)}")

    writer = csv.DictWriter(output,# dialect=csv.excel_tab,
                            fieldnames=header + ['path'])
    writer.writeheader()
    for rec in out.values():
        writer.writerow(rec)
    lgr.info("Saved output")


class LogLevel(click.ParamType):
    name = "log-level"
    levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]

    def convert(self, value, param, ctx):
        if value is None:
            return value
        try:
            return int(value)
        except ValueError:
            vupper = value.upper()
            if vupper in self.levels:
                return getattr(logging, vupper)
            else:
                self.fail(f"{value!r}: invalid log level", param, ctx)

    def get_metavar(self, param):
        return "[" + "|".join(self.levels) + "]"


def is_interactive():
    """Return True if all in/outs are tty"""
    # TODO: check on windows if hasattr check would work correctly and add value:
    #
    return sys.stdin.isatty() and sys.stdout.isatty() and sys.stderr.isatty()


def setup_exceptionhook(ipython=False):
    """Overloads default sys.excepthook with our exceptionhook handler.

       If interactive, our exceptionhook handler will invoke
       pdb.post_mortem; if not interactive, then invokes default handler.
    """

    def _pdb_excepthook(type, value, tb):
        import traceback

        traceback.print_exception(type, value, tb)
        print()
        if is_interactive():
            import pdb

            pdb.post_mortem(tb)

    if ipython:
        from IPython.core import ultratb

        sys.excepthook = ultratb.FormattedTB(
            mode="Verbose",
            # color_scheme='Linux',
            call_pdb=is_interactive(),
        )
    else:
        sys.excepthook = _pdb_excepthook


#
# Main group
#


def print_version(ctx, param, value):
    if not value or ctx.resilient_parsing:
        return
    click.echo(__version__)
    ctx.exit()


def upper(ctx, param, value):
    import pdb

    pdb.set_trace()
    return value.upper()


# group to provide commands
@click.group() # cls=DYMGroup)
@click.option(
    "--version", is_flag=True, callback=print_version, expose_value=False, is_eager=True
)
@click.option(
    "-l",
    "--log-level",
    help="Log level (case insensitive).  May be specified as an integer.",
    type=LogLevel(),
    default="INFO",
    show_default=True,
)
@click.option("--pdb", help="Fall into pdb if errors out", is_flag=True)
def main(log_level, pdb=False):
    """Helpers to support interfacing DataLad to NDA

    To see help for a specific command, run

        datalad-nda COMMAND --help

    """
    lgr.setLevel(log_level)
    if pdb:
        setup_exceptionhook()
    # try:
    #     import etelemetry
    #
    #     etelemetry.check_available_version("dandi/dandi-cli", __version__, lgr=lgr)
    # except Exception as exc:
    #     lgr.warning(
    #         "Failed to check for a more recent version available with etelemetry: %s",
    #         exc,
    #     )


main.add_command(condition4datalad)
main.add_command(submissions)

if __name__ == '__main__':
    main()