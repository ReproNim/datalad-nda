#!/usr/bin/env python3
"""A helper to condition manifest from NDA for download

Somehow, unlike with awk/sed monster, I do not see any where file is pointed to
by different urls!

TODOs:
- ?NDA check if submission ID is chronological (we sort by it and download only latest)
- ?NDA what happens when file is REMOVED from one submission to another?!
  i.e. how could we tell that it was an incremental submission or
  "full re-submission"?
- Add a mode to do it only for a specific submission id?
  Then we could establish chronology of uploads!  Then we can fail if
  within the same submission we get multiple URLs for the same file!
  May be that is best to do within datalad crawler!? otherwise we need multiple runs
  while they go through different submission IDs.
- add option to split into subdatasets e.g. for each subject (based on a regex)
  or should we do it while piping into `datalad addurls`?
"""

import click
import csv
import re

from collections import defaultdict

import logging
lgr = logging.getLogger(__file__)
logging.basicConfig(level=logging.INFO)


# To provide feedback on lengthy process of going through the large file
from tqdm import tqdm
import mmap
def get_num_lines(file_path):
    fp = open(file_path, "r+")
    buf = mmap.mmap(fp.fileno(), 0)
    lines = 0
    while buf.readline():
        lines += 1
    return lines


def cond1(row):
    """Basic conditioning of entries in row"""
    # we do not care about ""
    return [_.lstrip('"').rstrip('"') for _ in row]


@click.command()
@click.option('-i', '--input',
              default='-',
              help='Input "manifest" file from NDA.  Reads from sys.stdin if not provided',
              type=click.File('r'))
@click.option('-o', '--output',
              default='-',
              help='Output .csv file to feed DataLad with.  Dumped to stdout if not provided',
              type=click.File('w'))
def condition_manifest(input, output):
    url_regex = r's3://NDAR[^/]*/submission_[0-9]*/(?P<path>.*)'
    url_field = 'associated_file'

    lgr.info("Reading entire file")
    lines = list(input)
    #lines = full_input.split('\n')
    lgr.info("Read %d lines", len(lines))

    tbl_reader = csv.reader(lines, delimiter="\t")
    header = cond1(next(tbl_reader))
    assert url_field in header

    # We will retain full record just in case,
    # and just add explicit
    recs = []
    url_to_paths = defaultdict(set)  # just to check if some url (now) doesn't provide multiple files
    duplicates = defaultdict(list)  # just to keep internally list of all duplicates

    submission_ids = set()
    dataset_ids = set()

    for i, row in tqdm(enumerate(tbl_reader), total=len(lines), leave=False):
        if i == 0 and row == header:
            lgr.debug("Skipping 2nd line - identical to first")
            continue
        row = cond1(row)
        rec = dict(zip(header, row))
        rec['submission_id'] = int(rec['submission_id'])
        submission_ids.add(rec['submission_id'])
        dataset_ids.add(rec['dataset_id'])
        recs.append(rec)

    lgr.info(f"Loaded {len(recs)} records from {len(submission_ids)} submissions for {len(dataset_ids)} datasets.")
    # assert len(dataset_ids) == 1   # AFAIK
    lgr.info(f"Will sort (in reverse by submission_id) assuming that later submission ID is the most up to date one")
    recs = sorted(recs, key=lambda r: r["submission_id"], reverse=True)

    # in case of duplicates (the same url -> path) we keep only first hit record
    out = {}  # path: record.

    prev_submission_id = None
    for rec in tqdm(recs, leave=False):
        submission_id = rec['submission_id']
        if prev_submission_id is not None and prev_submission_id >= submission_id:
            raise ValueError(f"Not sorted by submission ID: {prev_submission_id} is followed by {submission_id}")

        url = rec[url_field]
        url_match = re.match(url_regex, url)
        if not url_match:
            raise ValueError(f"Cannot parse {url} using {url_regex!r}")
        path = url_match.groupdict()['path']
        if path in out:
            duplicates[path].append(rec) # [url_field])
            known_url = out[path][url_field]
            if known_url != url:
                lgr.debug(f'{path}: {url} != {known_url}')
                continue
            if path not in url_to_paths.get(url, set()):
                # we saw that url already! but it had a different path
                raise ValueError(f'{url} already provided {url_to_paths[url]}')

        url_to_paths[url].add(path)
        # for convenience store  that path straight in the rec
        rec['path'] = path
        out[path] = rec
    #multiurl_listing = '\n '.join(f' - {k}: {v}' for k, v in duplicates.items())
    lgr.info(f"Processed entire file: got {len(recs)} files with {len(duplicates)} files having multiple URLs to "
             f"possibly reach them")  # : {', '.join(duplicates)}")

    writer = csv.DictWriter(output,# dialect=csv.excel_tab,
                            fieldnames=header + ['path'])
    writer.writeheader()
    for rec in out.values():
        writer.writerow(rec)
    lgr.info("Saved output")


if __name__ == '__main__':
    condition_manifest()